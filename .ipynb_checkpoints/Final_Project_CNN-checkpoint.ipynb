{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulmathai/.virtualenvs/keras_tf/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(filepath):\n",
    "\tdataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
    "\treturn dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of files into a 3D array of [samples, timesteps, features]\n",
    "def load_group(filenames, prefix=''):\n",
    "\tloaded = list()\n",
    "\tfor name in filenames:\n",
    "\t\tdata = load_file(prefix + name)\n",
    "\t\tloaded.append(data)\n",
    "\t# stack group so that features are the 3rd dimension\n",
    "\tloaded = np.dstack(loaded)\n",
    "\treturn loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a dataset group, such as train or test\n",
    "def load_dataset_group(group, prefix=''):\n",
    "\tfilepath = prefix + group + '/Inertial Signals/'\n",
    "\t# load all 9 files as a single array\n",
    "\tfilenames = list()\n",
    "\t# total acceleration\n",
    "\tfilenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
    "\t# body acceleration\n",
    "\tfilenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
    "\t# body gyroscope\n",
    "\tfilenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
    "\t# load input data\n",
    "\tX = load_group(filenames, filepath)\n",
    "\t# load class output\n",
    "\ty = load_file(prefix + group + '/y_'+group+'.txt')\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset, returns train and test X and y elements\n",
    "def load_dataset(prefix=''):\n",
    "\t# load all train\n",
    "\ttrainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
    "\tprint(trainX.shape, trainy.shape)\n",
    "\t# load all test\n",
    "\ttestX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
    "\tprint(testX.shape, testy.shape)\n",
    "\t# zero-offset class values\n",
    "\ttrainy = trainy - 1\n",
    "\ttesty = testy - 1\n",
    "\t# one hot encode y\n",
    "\ttrainy = tf.keras.utils.to_categorical(trainy)\n",
    "\ttesty = tf.keras.utils.to_categorical(testy)\n",
    "\tprint(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
    "\treturn trainX, trainy, testX, testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainy, testX, testy = load_dataset()\n",
    "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data into time steps of sub-sequences\n",
    "n_steps, n_length = 4, 32\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu'), input_shape=(None,n_length,n_features)))\n",
    "model.add(TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu')))\n",
    "model.add(TimeDistributed(Dropout(0.5)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(n_outputs, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "\tprint(scores)\n",
    "\tm, s = mean(scores), std(scores)\n",
    "\tprint('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an experiment\n",
    "scores = list()\n",
    "def run_experiment(repeats=1):\n",
    "\t# load data\n",
    "\ttrainX, trainy, testX, testy = load_dataset()\n",
    "\t# repeat experiment\n",
    "\tfor r in range(repeats):\n",
    "\t\tscore = evaluate_model(trainX, trainy, testX, testy, model)\n",
    "\t\tscore = score * 100.0\n",
    "\t\tprint('>#%d: %.3f' % (r+1, score))\n",
    "\t\tscores.append(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_dir_cnn = \"logs/fitcnn/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir_cnn, histogram_freq=1, update_freq = 2000)\n",
    "def evaluate_model(trainX, trainy, testX, testy, model):\n",
    "\t# define model\n",
    "\tverbose, epochs, batch_size = 1, 25, 64\n",
    "\tn_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "\ttrainX = trainX.reshape((trainX.shape[0], n_steps, n_length, n_features))\n",
    "\ttestX = testX.reshape((testX.shape[0], n_steps, n_length, n_features))\n",
    "    # fit network\n",
    "\tmodel.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose, callbacks=[tensorboard_callback])\n",
    "\t# evaluate model\n",
    "\t_, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "\treturn accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7352, 128, 9) (7352, 1)\n",
      "(2947, 128, 9) (2947, 1)\n",
      "(7352, 128, 9) (7352, 6) (2947, 128, 9) (2947, 6)\n",
      "Train on 7352 samples\n",
      "Epoch 1/25\n",
      "7352/7352 [==============================] - 3s 444us/sample - loss: 0.0679 - accuracy: 0.9655\n",
      "Epoch 2/25\n",
      "7352/7352 [==============================] - 3s 422us/sample - loss: 0.0682 - accuracy: 0.9669\n",
      "Epoch 3/25\n",
      "7352/7352 [==============================] - 3s 417us/sample - loss: 0.0852 - accuracy: 0.9645\n",
      "Epoch 4/25\n",
      "7352/7352 [==============================] - 3s 417us/sample - loss: 0.0626 - accuracy: 0.9691\n",
      "Epoch 5/25\n",
      "7352/7352 [==============================] - 3s 452us/sample - loss: 0.0600 - accuracy: 0.9709\n",
      "Epoch 6/25\n",
      "7352/7352 [==============================] - 4s 489us/sample - loss: 0.0568 - accuracy: 0.9720\n",
      "Epoch 7/25\n",
      "7352/7352 [==============================] - 3s 404us/sample - loss: 0.0599 - accuracy: 0.9725\n",
      "Epoch 8/25\n",
      "7352/7352 [==============================] - 3s 404us/sample - loss: 0.0545 - accuracy: 0.9763\n",
      "Epoch 9/25\n",
      "7352/7352 [==============================] - 3s 410us/sample - loss: 0.0539 - accuracy: 0.9744\n",
      "Epoch 10/25\n",
      "7352/7352 [==============================] - 3s 445us/sample - loss: 0.0700 - accuracy: 0.9690\n",
      "Epoch 11/25\n",
      "7352/7352 [==============================] - 3s 450us/sample - loss: 0.0583 - accuracy: 0.9750\n",
      "Epoch 12/25\n",
      "7352/7352 [==============================] - 3s 445us/sample - loss: 0.0565 - accuracy: 0.9744\n",
      "Epoch 13/25\n",
      "7352/7352 [==============================] - 4s 479us/sample - loss: 0.0498 - accuracy: 0.9770\n",
      "Epoch 14/25\n",
      "7352/7352 [==============================] - 4s 477us/sample - loss: 0.0477 - accuracy: 0.9770\n",
      "Epoch 15/25\n",
      "7352/7352 [==============================] - 3s 425us/sample - loss: 0.0486 - accuracy: 0.9763\n",
      "Epoch 16/25\n",
      "7352/7352 [==============================] - 3s 468us/sample - loss: 0.0449 - accuracy: 0.9786\n",
      "Epoch 17/25\n",
      "7352/7352 [==============================] - 4s 485us/sample - loss: 0.0453 - accuracy: 0.9778\n",
      "Epoch 18/25\n",
      "7352/7352 [==============================] - 4s 533us/sample - loss: 0.0404 - accuracy: 0.9819\n",
      "Epoch 19/25\n",
      "7352/7352 [==============================] - 4s 575us/sample - loss: 0.0438 - accuracy: 0.9804\n",
      "Epoch 20/25\n",
      "7352/7352 [==============================] - 4s 515us/sample - loss: 0.0412 - accuracy: 0.9784\n",
      "Epoch 21/25\n",
      "7352/7352 [==============================] - 4s 503us/sample - loss: 0.0482 - accuracy: 0.9776\n",
      "Epoch 22/25\n",
      "7352/7352 [==============================] - 4s 550us/sample - loss: 0.0407 - accuracy: 0.9810\n",
      "Epoch 23/25\n",
      "7352/7352 [==============================] - 4s 536us/sample - loss: 0.0391 - accuracy: 0.9810\n",
      "Epoch 24/25\n",
      "7352/7352 [==============================] - 4s 512us/sample - loss: 0.0396 - accuracy: 0.9822\n",
      "Epoch 25/25\n",
      "7352/7352 [==============================] - 4s 484us/sample - loss: 0.0387 - accuracy: 0.9819\n",
      ">#1: 91.551\n",
      "[91.55073165893555]\n",
      "Accuracy: 91.551% (+/-0.000)\n"
     ]
    }
   ],
   "source": [
    "# run the experiment\n",
    "run_experiment()\n",
    "# summarize results\n",
    "summarize_results(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
